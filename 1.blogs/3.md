
@[TOC](近端策略优化（Proximal Policy Optimization）)

在强化学习（九）中介绍了REINFORCE算法。但REINFORCE算法还是存在三个问题：


1. 更新过程**效率很低**，我们运行一次策略，更新一次，然后丢弃该轨迹。
2. 梯度估计$g$是**嘈杂的**。随机收集的轨迹可能无法代表该策略。
3. 没有明确的**可信度赋值**。一个轨迹可能包含许多好的或者坏的动作，这些动作是否得到加强仅取决于最终的总产出。

在以下概念中，我们将探讨改进REINFORCE算法并解决所有3个问题的方法。所有改进将在PPO算法中利用和实现。
## 10.1 噪声消减
我们优化策略的方法是最大化平均奖励$U(\theta)$为了做到这一点，我们使用随机梯度上升。数学上，梯度是由所有可能轨迹的平均值给出的，

$$\nabla_\theta U(\theta) = \overbrace{\sum_\tau P(\tau; \theta)}^{ \begin{matrix} \scriptsize\textrm{average over}\\ \scriptsize\textrm{all trajectories} \end{matrix} } \underbrace{\left( R_\tau \sum_t \nabla_\theta \log \pi_\theta(a_t^{(\tau)}|s_t^{(\tau)}) \right)}_{ \textrm{only one is sampled} }\tag{10.1}$$
对于简单的问题，很容易有超过数百万条轨迹，而对于连续的问题，则有无限的轨迹。

在实际应用中，我们只需要取一个轨迹来计算梯度，然后更新我们的策略。所以，很多时候，采样轨迹的结果只是随机的，并不包含那么多关于我们策略的信息。那么如何进行学习呢？我们希望可以经过长时间的训练，将微小的信号积累起来。

要减少梯度中的噪声，最简单的选择就是简单地采样更多的轨迹。 利用分布式计算，我们可以并行收集多个轨迹，这样就不会花费太多时间。然后我们可以通过对所有不同轨迹的平均来估计策略梯度。



$$\left. \begin{matrix} s^{(1)}_t, a^{(1)}_t, r^{(1)}_t\\[6pt] s^{(2)}_t, a^{(2)}_t, r^{(2)}_t\\[6pt] s^{(3)}_t, a^{(3)}_t, r^{(3)}_t\\[6pt] \vdots \end{matrix} \;\; \right\}\!\!\!\! \rightarrow g = \frac{1}{N}\sum_{i=1}^N R_i \sum_t\nabla_\theta \log \pi_\theta(a^{(i)}_t | s^{(i)}_t) $$

**奖励归一化**
运行多个轨迹还有一个好处：我们可以收集所有的总奖励，并了解它们的分布情况。

在许多情况下，奖励的分布会随着学习的发生而变化。奖励$=1$可能在开始时非常好，但在1000个训练集之后就非常糟糕。

如果我们将奖励归一化，就可以提高学习效果，其中，$\mu$是平均值，$\sigma$是标准差。

$$R_i \leftarrow \frac{R_i -\mu}{\sigma} \qquad \mu = \frac{1}{N}\sum_i^N R_i \qquad \sigma = \sqrt{\frac{1}{N}\sum_i (R_i - \mu)^2}$$

(当所有的$R_i$都相同时，$\sigma=0$，我们可以将所有的归一化奖励设置为0，以避免数值问题)

这种批量归一化技术也用于人工智能中的许多其他问题（如图像分类），将输入归一化可以提高学习效果。

直观地说，将奖励归一化大致相当于挑选一半的行动来鼓励或者是不鼓励，同时也要确保梯度上升的步骤不要太大或者太小。

## 10.2 可信度赋值（Credit Assignment）
回到梯度估计，我们可以仔细看看总奖励$R$，它只是每一步的奖励之和$$R=r_1+r_2+...+r_{t-1}+r_t+...$$
$$g=\sum_t (...+r_{t-1}+r_{t}+...)\nabla_{\theta}\log \pi_\theta(a_t|s_t)$$
让我们来思考一下在时间步骤$t$发生了什么。即使在决定行动之前，智能体已经收到了直到步骤$t-1$的所有奖励。所以我们可以把总奖励中的这部分看作是过去的奖励。其余的部分则表示为未来的奖励。

$$(\overbrace{...+r_{t-1}}^{\cancel{R^{\rm past}_t}}+ \overbrace{r_{t}+...}^{R^{\rm future}_t})$$
因为我们有一个马尔科夫过程，时间步$t$的行动只能影响未来的奖励，所以过去的奖励不应该对策略梯度有贡献。

因此，为了正确地将可信度赋值给行动$a_t$。因此，我们应该忽略过去的奖励。所以更好的策略梯度应该是简单地将未来的奖励作为系数。

$$g=\sum_t R_t^{\rm future}\nabla_{\theta}\log \pi_\theta(a_t|s_t)$$ 

**梯度修改注意事项**
为什么只要改变我们的梯度就可以了呢？那岂不是改变了我们最初的目标--预期奖励最大化？事实证明，从数学上来说，忽略过去的奖励可能会改变每个具体轨迹的梯度，但不会改变平均梯度。所以，即使在训练过程中梯度不同，平均而言，我们仍然在最大化平均奖励。事实上，结果梯度的噪声较小，所以使用未来奖励进行训练应该会加快进度。
