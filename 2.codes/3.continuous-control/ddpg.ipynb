{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from unity_env import init_environment\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Unity env executable path\n",
    "UNITY_EXE_PATH = 'Reacher_Windows_x86_64/Reacher.exe'\n",
    "# Environment Goal\n",
    "GOAL = 30.1\n",
    "# Averaged score\n",
    "SCORE_AVERAGED = 100\n",
    "# Let us know the progress each 10 timesteps\n",
    "PRINT_EVERY = 10\n",
    "# Number of episode for training\n",
    "N_EPISODES = 160\n",
    "# Max Timesteps\n",
    "MAX_TIMESTEPS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from actor import Actor\n",
    "from critic import Critic\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from replay_buffer import ReplayBuffer\n",
    "\n",
    "# Replay Buffer Size\n",
    "BUFFER_SIZE = int(1e6)\n",
    "# Minibatch Size\n",
    "BATCH_SIZE = 256 \n",
    "# Discount Gamma\n",
    "GAMMA = 0.995 \n",
    "# Soft Update Value\n",
    "TAU = 1e-2   \n",
    "# Learning rates for each NN      \n",
    "LR_ACTOR = 1e-3 \n",
    "LR_CRITIC = 1e-3\n",
    "# Update network every X timesteps\n",
    "UPDATE_EVERY = 32\n",
    "# Learn from batch of experiences n_experiences times\n",
    "N_EXPERIENCES = 16   \n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "class DDPG():\n",
    "    \"\"\"Interacts with and learns from the environment using the DDPG algorithm.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, random_seed):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # Actor Neural Network (Regular and target)\n",
    "        self.actor_regular = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_regular.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Neural Network (Regular and target)\n",
    "        self.critic_regular = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_regular.parameters(), lr=LR_CRITIC)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "          \n",
    "        # Ensure that both networks have the same weights\n",
    "        self.deep_copy(self.actor_target, self.actor_regular)\n",
    "        self.deep_copy(self.critic_target, self.critic_regular)\n",
    "\n",
    "    def step(self, states, actions, rewards, next_states, dones, timestep):\n",
    "        # Save collected experiences\n",
    "        for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\n",
    "            self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn from our buffer if possible\n",
    "        if len(self.memory) > BATCH_SIZE and timestep % UPDATE_EVERY == 0:\n",
    "            for _ in range(N_EXPERIENCES):\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, states):\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        \n",
    "        # Evaluation mode\n",
    "        # Notify all your layers that you are in eval mode, that way, \n",
    "        # Batchnorm or dropout layers will work in eval mode instead of training mode.\n",
    "        self.actor_regular.eval()\n",
    "        # torch.no_grad() impacts the autograd engine and deactivate it. \n",
    "        # It will reduce memory usage and speed up\n",
    "        with torch.no_grad():\n",
    "            actions = self.actor_regular(states).cpu().data.numpy()\n",
    "        # Enable Training mode\n",
    "        self.actor_regular.train()\n",
    "\n",
    "        return actions\n",
    "\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Update the critic neural network\n",
    "        # Get predicted next-state actions\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        # Get Q values from target model\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "\n",
    "        # Compute Q targets for current states\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        # Calculate the critic loss\n",
    "        Q_expected = self.critic_regular(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Update the actor neural network\n",
    "        # Calculate the actor loss\n",
    "        actions_pred = self.actor_regular(states)\n",
    "        # Change sign because of the gradient descent\n",
    "        actor_loss = -self.critic_regular(states, actions_pred).mean()\n",
    "\n",
    "        # Minimize the loss function\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Update target network using the soft update approach (slowly updating)\n",
    "        self.soft_update(self.critic_regular, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_regular, self.actor_target, TAU)\n",
    "\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        # Update the target network slowly to improve the stability\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau) * target_param.data)\n",
    "\n",
    "    def deep_copy(self, target, source):\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "\n",
    "def init_environment(executable_path): \n",
    "    \n",
    "    # Init the Reacher Unity Environment\n",
    "    env = UnityEnvironment(file_name=executable_path)\n",
    "    # Get the default brain\n",
    "    brain_name = env.brain_names[0]\n",
    "    brain = env.brains[brain_name]\n",
    "    # Reset the environment\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    # Number of agents\n",
    "    n_agents = len(env_info.agents)\n",
    "    # Size of each action\n",
    "    action_size = brain.vector_action_space_size\n",
    "    \n",
    "    # Get state space \n",
    "    states = env_info.vector_observations\n",
    "    state_size = states.shape[1]\n",
    "    \n",
    "    return env, brain_name, n_agents, state_size, action_size\n",
    "\n",
    "\n",
    "\n",
    "# Init the reacher environment and get agents, state and action info\n",
    "env, brain_name, n_agents, state_size, action_size = init_environment(UNITY_EXE_PATH)\n",
    "agent = DDPG(state_size=state_size, action_size=action_size, random_seed=89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tAverage Score: 0.76\n",
      "Episode 20\tAverage Score: 1.39\n",
      "Episode 30\tAverage Score: 2.67\n",
      "Episode 40\tAverage Score: 5.45\n",
      "Episode 50\tAverage Score: 9.58\n",
      "Episode 60\tAverage Score: 13.66\n",
      "Episode 70\tAverage Score: 17.16\n",
      "Episode 80\tAverage Score: 19.90\n",
      "Episode 90\tAverage Score: 22.05\n",
      "Episode 100\tAverage Score: 23.73\n",
      "Episode 110\tAverage Score: 27.14\n",
      "\n",
      "Environment solved in 119 episodes!\tAverage Score: 30.33\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\" # this avoids the crush of the ipykernel kernel\n",
    "\n",
    "\n",
    "#  Method for training the agent\n",
    "def train(n_episodes=N_EPISODES):\n",
    "    scores_deque = deque(maxlen=SCORE_AVERAGED)\n",
    "    global_scores = []\n",
    "    averaged_scores = []\n",
    "    \n",
    "    for episode in range(1, N_EPISODES + 1):\n",
    "        # Get the current states for each agent\n",
    "        states = env.reset(train_mode=True)[brain_name].vector_observations \n",
    "        # Init the score of each agent to zeros\n",
    "        scores = np.zeros(n_agents)                \n",
    "\n",
    "        for t in range(MAX_TIMESTEPS):\n",
    "\n",
    "            # Act according to our policy\n",
    "            actions = agent.act(states)\n",
    "            # Send the decided actions to all the agents\n",
    "            env_info = env.step(actions)[brain_name]        \n",
    "            # Get next state for each agent\n",
    "            next_states = env_info.vector_observations     \n",
    "            # Get rewards obtained from each agent\n",
    "            rewards = env_info.rewards           \n",
    "            # Info about if an env is done\n",
    "            dones = env_info.local_done   \n",
    "            # Learn from the collected experience\n",
    "            agent.step(states, actions, rewards, next_states, dones, t)\n",
    "            # Update current states\n",
    "            states = next_states   \n",
    "            # Add the rewards recieved\n",
    "            scores += rewards    \n",
    "            \n",
    "            # Stop the loop if an agent is done               \n",
    "            if np.any(dones):                          \n",
    "                break\n",
    "        \n",
    "        # Calculate scores and averages\n",
    "        score = np.mean(scores)\n",
    "        scores_deque.append(score)\n",
    "        avg_score = np.mean(scores_deque)\n",
    "        \n",
    "        global_scores.append(score)\n",
    "        averaged_scores.append(avg_score)\n",
    "                \n",
    "        if episode % PRINT_EVERY == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_deque)))  \n",
    "            \n",
    "        if avg_score >= GOAL:  \n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(episode, avg_score))\n",
    "            torch.save(agent.actor_regular.state_dict(), 'actor_theta.pth')\n",
    "            torch.save(agent.critic_regular.state_dict(), 'critic_theta.pth')\n",
    "            break\n",
    "            \n",
    "    return global_scores, averaged_scores\n",
    "\n",
    "# Train the agent and get the results\n",
    "scores, averages = train()\n",
    "\n",
    "# Plot Statistics (Global scores and averaged scores)\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(np.arange(1, len(scores) + 1), averages)\n",
    "plt.ylabel('Reacher Environment Average Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udacity-py36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
