{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='reacher/Reacher.exe', no_graphics=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReacherBrain\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "print(brain_name)\n",
    "print(brain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.19999999552965164\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\" # this avoids the crush of the ipykernel kernel\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# check which device is being used. \n",
    "# I recommend disabling gpu until you've made sure that the code runs\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=256, fc2_units=128):\n",
    "        super(Policy, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x))\n",
    "    \n",
    "policy = Policy(state_size, action_size, 0)\n",
    "\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(policy.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.18789206  0.18410847 -0.00372682  0.13148358]]\n",
      "[[-0.33759245  0.02613708  0.50955511 -0.08898609]]\n"
     ]
    }
   ],
   "source": [
    "# action test\n",
    "actions = policy(torch.tensor(states, dtype=torch.float)).detach().numpy()\n",
    "print(actions)\n",
    "actions = np.random.randn(num_agents, action_size)\n",
    "actions = np.clip(actions, -1, 1)\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def clipped_surrogate(policy, old_probs, states, actions, rewards,\n",
    "                      discount = 0.995, epsilon=0.1, beta=0.01):\n",
    "\n",
    "    discount = discount**np.arange(len(rewards))\n",
    "    rewards = np.asarray(rewards)*discount[:,np.newaxis]\n",
    "    print(\"rewards\", rewards)\n",
    "    \n",
    "    # convert rewards to future rewards e.g. [r0, r1, r2, r3] -> [r0 + r1 + r2 + r3, r1 + r2 + r3, r2 + r3, r3]\n",
    "    rewards_future = rewards[::-1].cumsum(axis=0)[::-1]\n",
    "    print(\"rewards_future\", rewards_future)\n",
    "    \n",
    "    mean = np.mean(rewards_future, axis=1)\n",
    "    std = np.std(rewards_future, axis=1) + 1.0e-10\n",
    "\n",
    "    rewards_normalized = ((rewards_future - mean[:,np.newaxis])/std[:,np.newaxis])\n",
    "    print(\"rewards_normalized\", rewards_normalized)\n",
    "    \n",
    "    # convert everything into pytorch tensors and move to gpu if available\n",
    "    actions = torch.tensor(actions, dtype=torch.int8, device=device)\n",
    "    old_probs = torch.tensor(old_probs, dtype=torch.float, device=device)\n",
    "    R = torch.tensor(rewards_normalized, dtype=torch.float, device=device)\n",
    "    print(\"R\", R)\n",
    "\n",
    "    # convert states to policy (or probability)\n",
    "    new_probs = policy(torch.tensor(states, dtype=torch.float)).to(device)\n",
    "\n",
    "    # new_probs = torch.where(actions == pong_utils.RIGHT, new_probs, 1.0-new_probs)\n",
    "\n",
    "    ratio = new_probs/old_probs+1.e-10\n",
    "    clipped_ratio = torch.clamp(ratio, 1-epsilon, 1+epsilon)\n",
    "    loss = torch.min(ratio*R, clipped_ratio*R)\n",
    "    print(\"loss\", loss)\n",
    "\n",
    "    # include a regularization term\n",
    "    # this steers new_policy towards 0.5\n",
    "    # prevents policy to become exactly 0 or 1 helps exploration\n",
    "    # add in 1.e-10 to avoid log(0) which gives nan\n",
    "    entropy = -(new_probs*torch.log(old_probs+1.e-10)+ \\\n",
    "        (1.0-new_probs)*torch.log(1.0-old_probs+1.e-10))\n",
    "\n",
    "    return torch.mean(loss + beta*entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards [[0.]]\n",
      "rewards_future [[0.]]\n",
      "rewards_normalized [[0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\97575\\anaconda3\\envs\\udacity-py36\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.]]\n",
      "rewards_future [[0.]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.]]\n",
      "rewards_future [[0.]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.]]\n",
      "rewards_future [[0.]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.]]\n",
      "rewards_future [[0.]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.]]\n",
      "rewards_future [[0.]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.]]\n",
      "rewards_future [[0.]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.]]\n",
      "rewards_future [[0.]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.]]\n",
      "rewards_future [[0.]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.]]\n",
      "rewards_future [[0.]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.]]\n",
      "rewards_future [[0.]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.]]\n",
      "rewards_future [[0.]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.]]\n",
      "rewards_future [[0.]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.]]\n",
      "rewards_future [[0.]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.]]\n",
      "rewards_future [[0.]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.]]\n",
      "rewards_future [[0.]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.]]\n",
      "rewards_future [[0.]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.]]\n",
      "rewards_future [[0.]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.]]\n",
      "rewards_future [[0.]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.]]\n",
      "rewards_future [[0.]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.]]\n",
      "rewards_future [[0.]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.]]\n",
      "rewards_future [[0.]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.]]\n",
      "rewards_future [[0.]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.]]\n",
      "rewards_future [[0.]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.]]\n",
      "rewards_future [[0.]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.]]\n",
      "rewards_future [[0.]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.]]\n",
      "rewards_future [[0.]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.]]\n",
      "rewards_future [[0.]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.04]]\n",
      "rewards_future [[0.04]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.08]]\n",
      "rewards_future [[0.08]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.12]]\n",
      "rewards_future [[0.12]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.16]]\n",
      "rewards_future [[0.16]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.2]]\n",
      "rewards_future [[0.2]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.23999999]]\n",
      "rewards_future [[0.23999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.27999999]]\n",
      "rewards_future [[0.27999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.31999999]]\n",
      "rewards_future [[0.31999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.34999999]]\n",
      "rewards_future [[0.34999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.37999999]]\n",
      "rewards_future [[0.37999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.41999999]]\n",
      "rewards_future [[0.41999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.45999999]]\n",
      "rewards_future [[0.45999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.49999999]]\n",
      "rewards_future [[0.49999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.51999999]]\n",
      "rewards_future [[0.51999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.55999999]]\n",
      "rewards_future [[0.55999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.59999999]]\n",
      "rewards_future [[0.59999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.63999999]]\n",
      "rewards_future [[0.63999999]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.67999998]]\n",
      "rewards_future [[0.67999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.71999998]]\n",
      "rewards_future [[0.71999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.75999998]]\n",
      "rewards_future [[0.75999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.79999998]]\n",
      "rewards_future [[0.79999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., -0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., -0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., -0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., -0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., -0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., -0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., -0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., -0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., -0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., -0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., -0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., -0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., -0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., -0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., -0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., -0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., -0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., -0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., -0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., -0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., -0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., -0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., -0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., -0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., -0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., -0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., -0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., -0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., -0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., -0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., -0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., -0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop: 100%|| 1/1 [00:08<00:00,  8.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., -0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[0., 0., 0., 0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "rewards [[0.83999998]]\n",
      "rewards_future [[0.83999998]]\n",
      "rewards_normalized [[0.]]\n",
      "R tensor([[0.]], device='cuda:0')\n",
      "loss tensor([[-0., -0., -0., -0.]], device='cuda:0', grad_fn=<MinimumBackward>)\n",
      "[0.83999998]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# training loop max iterations\n",
    "episode = 1\n",
    "SGD_epoch = 1\n",
    "\n",
    "with tqdm(total=episode, desc=\"training loop\") as pbar:\n",
    "    for e in range(episode):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        while True:\n",
    "            actions = policy(torch.tensor(states, dtype=torch.float)).detach().numpy()\n",
    "            old_probs = policy(torch.tensor(states, dtype=torch.float))\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            scores += env_info.rewards                         # update the score (for each agent)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "\n",
    "            total_rewards = np.sum(rewards, axis=0)\n",
    "\n",
    "            # gradient ascent step\n",
    "            for _ in range(SGD_epoch):\n",
    "                L = -clipped_surrogate(policy, old_probs, states, actions, scores, epsilon=0.1, beta=0.01)\n",
    "                # if L is not zero\n",
    "                # if L != 0:\n",
    "                #     optimizer.zero_grad()\n",
    "                #     L.backward()\n",
    "                #     optimizer.step()\n",
    "                #     del L\n",
    "\n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                break\n",
    "                # update progress bar\n",
    "        if (e + 1) % 1 == 0:\n",
    "            # print(\"Episode: {0:d}, score: {1:f}\".format(e + 1, scores))\n",
    "            print(scores)\n",
    "        pbar.update(1)\n",
    "    # print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
