{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup a game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConnectN import ConnectN\n",
    "\n",
    "# Initialize input board size and the winning condition\n",
    "game_setting = {'size':(3,3), 'N':3}\n",
    "\n",
    "# Initialize a ConnectN object\n",
    "game = ConnectN(**game_setting)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "-1\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "game.move((0,1))\n",
    "print(game.state)\n",
    "print(game.player) # first player 1, and second player -1\n",
    "print(game.score) # keep track socre(game is not over: None, draw: 0, first player won: 1, second player won: -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.  1.  0.]\n",
      " [-1.  1.  0.]\n",
      " [ 0.  1.  0.]]\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# player -1 move\n",
    "game.move((0,0))\n",
    "# player +1 move\n",
    "game.move((1,1))\n",
    "# player -1 move\n",
    "game.move((1,0))\n",
    "# player +1 move\n",
    "game.move((2,1))\n",
    "\n",
    "print(game.state)\n",
    "print(game.player)\n",
    "print(game.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play a game interactively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt\n",
    "\n",
    "from Play import Play\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\" # this avoids the crush of the ipykernel kernel\n",
    "\n",
    "\n",
    "# when player1 or player2 is None, it means they are from mouse click.\n",
    "gameplay=Play(ConnectN(**game_setting), \n",
    "              player1=None, \n",
    "              player2=None)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize an AI to play the game\n",
    "We need to define a policy for tic-tac-toe, that takes the game state as input, and outputs a policy and a critic\n",
    "\n",
    "## Tentative Exercise:\n",
    "Code up your own policy for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy(\n",
      "  (conv): Conv2d(1, 16, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
      "  (fc): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc_action1): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc_action2): Linear(in_features=16, out_features=9, bias=True)\n",
      "  (fc_value1): Linear(in_features=32, out_features=8, bias=True)\n",
      "  (fc_value2): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (tanh_value): Tanh()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import *\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        NN contains both the policy and the critic\n",
    "        \n",
    "        1 input image channel, 16 output channels/feaure map(filter)\n",
    "        output_dim = (W-F+2P)/S + 1\n",
    "        \n",
    "        Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True)\n",
    "        Linear(in_features, out_features, bias=True)\n",
    "        '''\n",
    "        self.conv = nn.Conv2d(1, 16, kernel_size=2, stride=1, bias=False)\n",
    "        self.size = 2*2*16 # input features: 16 outputs 2x2 filtered map size\n",
    "        self.fc = nn.Linear(self.size,32)\n",
    "\n",
    "        # layers for the policy\n",
    "        self.fc_action1 = nn.Linear(32, 16)\n",
    "        self.fc_action2 = nn.Linear(16, 9)\n",
    "        \n",
    "        # layers for the critic\n",
    "        self.fc_value1 = nn.Linear(32, 8)\n",
    "        self.fc_value2 = nn.Linear(8, 1)\n",
    "        self.tanh_value = nn.Tanh()\n",
    "        \n",
    "    # feedforward behavior\n",
    "    def forward(self, x):\n",
    "\n",
    "        '''\n",
    "        # Apply [CONV-RELU]\n",
    "        # Flattening used for the output of conv/pooling layer to a linear layer (vector)\n",
    "        '''\n",
    "        y = F.relu(self.conv(x))   # Apply [CONV-RELU]\n",
    "        y = y.view(-1, self.size)  # Flattening used for the output of conv/pooling layer to a linear layer\n",
    "        y = F.relu(self.fc(y))     # One linear layer\n",
    "        \n",
    "        # you can easily use a softmax on a 9x9 matrix to get the prob. but this doesn't work quite that well.\n",
    "        # Given that not all the moves are available because say, player one plays a piece in a position,\n",
    "        # then player negative one could not place another piece in that same location.\n",
    "        # To fix it, compute 'avial' called the availablility matrix, which basically gives zeros when\n",
    "        # the move is unavailable in that location and it gives one when a move is available.\n",
    "        # the action head\n",
    "        a = F.relu(self.fc_action1(y))\n",
    "        a = self.fc_action2(a)\n",
    "        \n",
    "        # availability of moves\n",
    "        avail = (torch.abs(x.squeeze())!=1).type(torch.FloatTensor)\n",
    "        avail = avail.reshape(-1,9)\n",
    "#         avail = avail.view(-1, 9)\n",
    "        \n",
    "        # locations where actions are not possible, we set the prob to zero\n",
    "        maxa = torch.max(a)\n",
    "        \n",
    "        # Softmax operation.\n",
    "        # Doin this below will ensure that when a move is not legal, the prob will be exactly zero.\n",
    "        # subtract off max for numerical stability (avoids blowing up at infinity)\n",
    "        exp = avail*torch.exp(a-maxa)\n",
    "        prob = exp/torch.sum(exp)\n",
    "        \n",
    "        # critic\n",
    "        # the value head\n",
    "        value = F.relu(self.fc_value1(y))\n",
    "        value = self.tanh_value(self.fc_value2(value))\n",
    "        return prob.view(3,3), value\n",
    "\n",
    "# # we use the adam optimizer with learning rate 2e-4\n",
    "# # optim.SGD is also possible\n",
    "# import torch.optim as optim\n",
    "policy = Policy()\n",
    "# optimizer = optim.Adam(policy.parameters(), lr=1.e-4, weight_decay=1.e-4)\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a player that uses MCTS and the expert policy + critic to play a game\n",
    "\n",
    "We've introduced a new parameter\n",
    "$T$ = temperature\n",
    "\n",
    "This tells us how to choose the next move based on the MCTS results\n",
    "\n",
    "$$p_a = \\frac{N_a^{\\frac{1}{T}}}{\\sum_a N_a^{\\frac{1}{T}}}$$\n",
    "\n",
    "$T \\rightarrow 0$, we choose action with largest $N_a$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MCTS\n",
    "\n",
    "from copy import copy\n",
    "import random\n",
    "\n",
    "def Policy_Player_MCTS(game):\n",
    "    mytree = MCTS.Node(copy(game)) # make a copy and initialize a MCTS class\n",
    "    for _ in range(50):\n",
    "        mytree.explore(policy) # Compute all the U's, pick the brach with maximal U, search, expand, back-prop and then increase the viscalc\n",
    "   \n",
    "    mytreenext, (v, nn_v, p, nn_p) = mytree.next(temperature=0.1) # tell the tree to choose a next move\n",
    "        \n",
    "    return mytreenext.game.last_move\n",
    "\n",
    "def Random_Player(game):\n",
    "    return random.choice(game.available_moves())    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-489cce3e1a49>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mgame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConnectN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mgame_setting\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mPolicy_Player_MCTS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-057da00a43d7>\u001b[0m in \u001b[0;36mPolicy_Player_MCTS\u001b[1;34m(game)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mmytree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMCTS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# make a copy and initialize a MCTS class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mmytree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Compute all the U's, pick the brach with maximal U, search, expand, back-prop and then increase the viscalc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mmytreenext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnn_v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnn_p\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmytree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# tell the tree to choose a next move\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ML_Projects\\RL_review\\2.codes\\4.AlphaZero\\MCTS.py\u001b[0m in \u001b[0;36mexplore\u001b[1;34m(self, policy)\u001b[0m\n\u001b[0;32m    156\u001b[0m             \u001b[1;31m# policy outputs results from the perspective of the next player\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[1;31m# thus extra - sign is needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m             \u001b[0mnext_actions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m             \u001b[0mcurrent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[0mcurrent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_child\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_actions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ML_Projects\\RL_review\\2.codes\\4.AlphaZero\\MCTS.py\u001b[0m in \u001b[0;36mprocess_policy\u001b[1;34m(policy, game)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[0mframe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m     \u001b[0mprob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m     \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavailable_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\97575\\anaconda3\\envs\\udacity-py36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-4335625a6de1>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;31m# Doin this below will ensure that when a move is not legal, the prob will be exactly zero.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;31m# subtract off max for numerical stability (avoids blowing up at infinity)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0mexp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mavail\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mmaxa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m         \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexp\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "game = ConnectN(**game_setting)\n",
    "print(game.state)\n",
    "Policy_Player_MCTS(game)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play a game against the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % matplotlib notebook\n",
    "\n",
    "\n",
    "gameplay=Play(ConnectN(**game_setting), \n",
    "              player1=None, \n",
    "              player2=Policy_Player_MCTS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our alphazero agent and optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "game=ConnectN(**game_setting)\n",
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=.01, weight_decay=1.e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tenative exercise:\n",
    "code up the alphazero loss function, defined to be\n",
    "$$L = \\sum_t \\left\\{ (v^{(t)}_\\theta - z)^2  - \\sum_a p^{(t)}_a \\log \\pi_\\theta(a|s_t) \\right\\} + \\textrm{constant}$$ \n",
    "I added a constant term $\\sum_t \\sum_a p^{(t)}\\log p^{(t)}$ so that when $v_\\theta^{(t)} = z$ and $p^{(t)}_a = \\pi_\\theta(a|s_t)$, $L=0$, this way we can have some metric of progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mstan_fu\u001b[0m (\u001b[33mstanfu\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\ML_Projects\\RL_review\\2.codes\\4.AlphaZero\\wandb\\run-20240603_204301-mg92h3yc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/stanfu/alpha-zero-training/runs/mg92h3yc' target=\"_blank\">alpha-zero</a></strong> to <a href='https://wandb.ai/stanfu/alpha-zero-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/stanfu/alpha-zero-training' target=\"_blank\">https://wandb.ai/stanfu/alpha-zero-training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/stanfu/alpha-zero-training/runs/mg92h3yc' target=\"_blank\">https://wandb.ai/stanfu/alpha-zero-training/runs/mg92h3yc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 800/800 [01:56<00:00,  6.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▅▆▅▅▄▇▅▂▆▂▂▂▃▁▃▂▆▁▂▁▂▁▁▁▂▂▁▁▂▁▁▂▂█▁▁▁▆▁</td></tr><tr><td>outcome</td><td>▁██▅▅▅██▅█▅▅▅▅▅▅▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▅▅▅█▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>800</td></tr><tr><td>loss</td><td>0.51857</td></tr><tr><td>outcome</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">alpha-zero</strong> at: <a href='https://wandb.ai/stanfu/alpha-zero-training/runs/mg92h3yc' target=\"_blank\">https://wandb.ai/stanfu/alpha-zero-training/runs/mg92h3yc</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240603_204301-mg92h3yc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "import MCTS\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project=\"alpha-zero-training\", name=\"alpha-zero\")\n",
    "\n",
    "episodes = 800\n",
    "outcomes = []\n",
    "losses = []\n",
    "\n",
    "with tqdm(total=episodes, desc=\"Training Progress\") as pbar:\n",
    "    for e in range(episodes):\n",
    "        # each episode, initialize a top node for the MCTS\n",
    "        mytree = MCTS.Node(ConnectN(**game_setting))\n",
    "        \n",
    "        vterm = []\n",
    "        logterm = []\n",
    "        \n",
    "        # whenever the game is not over yet, explore the tree 50 steps\n",
    "        while mytree.outcome is None:\n",
    "            for _ in range(50):\n",
    "                mytree.explore(policy)\n",
    "            \n",
    "            current_player = mytree.game.player  # keep track of the player\n",
    "            \n",
    "            # v: expected outcome computed from the tree search\n",
    "            # nn_v: the critic value evaluating the current board\n",
    "            # p: a list of prob of taking each action computed from MCTS\n",
    "            # nn_p: similar to p but from the policy network directly\n",
    "            mytree, (v, nn_v, p, nn_p) = mytree.next()  # increment to the next tree\n",
    "            mytree.detach_mother()\n",
    "            \n",
    "            # Compute the loss function by comparing p with nn_p and nn_v with the actual outcome\n",
    "            loglist = torch.log(nn_p) * p  # part of the loss function\n",
    "            constant = torch.where(p > 0, p * torch.log(p), torch.tensor(0.))\n",
    "            logterm.append(-torch.sum(loglist - constant))\n",
    "            \n",
    "            vterm.append(nn_v * current_player)\n",
    "            \n",
    "        # Compute the \"policy_loss\" for computing gradient\n",
    "        outcome = mytree.outcome\n",
    "        outcomes.append(outcome)\n",
    "        \n",
    "        loss = torch.sum((torch.stack(vterm) - outcome) ** 2 + torch.stack(logterm))\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        losses.append(float(loss))\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log metrics to wandb\n",
    "        wandb.log({\"episode\": e + 1, \"loss\": float(loss), \"outcome\": outcome})\n",
    "        \n",
    "        # Update tqdm progress bar\n",
    "        pbar.update(1)\n",
    "        \n",
    "        del loss\n",
    "\n",
    "# Finish wandb run\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot your losses\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# % matplotlib notebook\n",
    "plt.plot(losses)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play a game against your alphazero agent !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\97575\\anaconda3\\envs\\udacity-py36\\lib\\site-packages\\matplotlib\\cbook\\__init__.py\", line 224, in process\n",
      "    func(*args, **kwargs)\n",
      "  File \"c:\\ML_Projects\\RL_review\\2.codes\\4.AlphaZero\\Play.py\", line 141, in click\n",
      "    loc = self.player1(self.game)\n",
      "TypeError: 'NoneType' object is not callable\n"
     ]
    }
   ],
   "source": [
    "# as first player\n",
    "gameplay=Play(ConnectN(**game_setting), \n",
    "              player1=None, \n",
    "              player2=Policy_Player_MCTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "# as second player\n",
    "\n",
    "gameplay=Play(ConnectN(**game_setting), \n",
    "              player2=None, \n",
    "              player1=Policy_Player_MCTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "# AI vs AI\n",
    "\n",
    "gameplay=Play(ConnectN(**game_setting), \n",
    "              player2=Policy_Player_MCTS, \n",
    "              player1=Policy_Player_MCTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
